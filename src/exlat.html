<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<title>reveal.js</title>

<link rel="stylesheet" href="../css/reveal.css">
<link rel="stylesheet" href="../css/theme/white.css">
<link rel="stylesheet" href="../css/local.css">

<!-- Theme used for syntax highlighting of code -->
<link rel="stylesheet" href="../lib/css/vs.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? '../css/print/pdf.css' : '../css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
</head>

<!-- Start of presentation --> 
<body>
<div class="reveal">
<div class="slides">

  <section>

    <h1 style = "text-transform: none !important">ExaLAT</h1>
    <!--
    <p>Kevin Stratford</p>
    <p> kevin@epcc.ed.ac.uk </p> -->
    <br>
    <p>Material by: Nick Johnson </p>
    <img class = "plain" src ="../img/epcc_logo.png" alt = "EPCC Logo" />
  </section>

  <section>
    <h3>Why should we be interested in GPUs?</h3>

    <ul>
    <li> Massively parallel performance </li>
    <ul class = "inner">
        <li> Very good at BLAS-like operations </li>
        <li> Established hardware available in many places</li>
        <li> Established programming language with lots of examples, help etc. </li>
        <li> Good suite of tools, compilers, profilers etc. </li>
		<li> C, C++ and Fortran (plus others) supported by vendor </li>
    </ul>
    <li> Why wouldn't you want to use CUDA </li>
    <ul class = "inner">
       <li> Low but not zero bar to entry</li>
       <li> Not "open" and not necessarily available on Intel/AMD hardware</li>
	   <li> For a small code, or a quick test of something, that low bar can be high for a new user</li>
	   <li> Can be a bit "boiler plate" in places compared to, for example, directives</li>   
    </ul>
    </ul>
  </section>

  <section>
    <h4> Example Application: Conjugate Gradient  </h4>

    <ul>
    <li> How to exploit the available resources: </li>
    <ul class = "inner">
       <li> Make sure you can decompose the problem across GPUs</li>
       <li> Ideally, make it decomposable across blocks and threads </li>
	   <li> Minimise any halo swapping operations</li>
    </ul>
    <li> The ideal code can adapt to use N GPUs at runtime</li>
    <ul class = "inner">
       <li> You don't want to have to tweak the code for each set of resources </li>
       <li> Multiple ways to do this</li>
	   <li> ... of escalating complexity </li>
	   <li> Having the host code in a good shape, then making sure you have a multi-block GPU code in good shape on a single GPU helps a lot!</li>
    </ul>
    </ul>

  </section>
  
  <section>
  <h4> Example Application: Conjugate Gradient  </h4>

  <ul>
  <li> Today's exercises are based around building a conjugate gradient implementation. </li>
  <ul class = "inner">
    <li> What is CG?</li>
    <li> Effectively an interative algorithm to find x given a system Ax = b </li>
	<li> Some may be familiar with this algorithm and variations thereof </li>
  </ul>
  <li> We will look at the most basic version</li>
  <li> This does not involve any preconditioners etc</li>
  <li> A is a simple sqaure matrix with values between 0 and 1 on the leading diagonal</li>
  <li> x is all zeros and b is all ones</li>
  </ul>
  
  </section>
  
  

  <section>
    <h4> Overarching theme of today's exercises. </h4>
    <div class="lblock">
      <img class = "plain" src = "./conjgradfromwikipedia.svg" alt = "Basic conjugate gradient algorithm image from Wikipedia"/><br>
	  Img from: https://en.wikipedia.org/wiki/Conjugate_gradient_method	   
    </div>

    <div class="rblock">
      <ol>
		<li> Basic CG </li>
        <li> On multiple-GPUs </li>
		<li> Optimally </li>
		<li> Remotely </li>
      </ol>
    </div>
	
	</section>
	
	<section>
    <h4> Overarching theme of today's exercises. </h4>
    <div class="lblock">
      <img class = "plain" src = "./conjgradfromwikipedia.svg" alt = "Basic conjugate gradient algorithm image from Wikipedia"/><br>
      Img from: https://en.wikipedia.org/wiki/Conjugate_gradient_method	        
    </div>

    <div class="rblock">
      <ol>
		<li> Let's break it down into some smaller blocks. </li>
        <li> We've got some basic building blocks here </li>
		<li> They are standard mathematical functions which we should be able to do on paper </li>
		<li> There are 3 different classes of function, what are they? </li>

      </ol>
    </div>
	
	</section>

	
	<section>
	<h4> How to begin...</h4>
	<ul>
    <li>Let's start with the most basic building blocks</li>
	<li>If we implement these, we can begin to build up our arsenal of kernels that we can use to construct our vanilla-CG algorithm.</li>
	</ul>
	
	<div class="lblock">
    <ul>
       <li> Vector something Vector
		<ul>
			<li> Scale by a value </li>
			<li> Add / Subtract </li>
			<li> Vector Product </li>
			<li> Transpose </li>
	   </ul>
	   </li>
   </ul>
   </div>
   
	<div class="rblock">
	<ul>	
       <li> Matrix-Vector</li>
	   
       <li> Single value computation
	   <ul>
			<li> Reduction </li>
			<li> Max/Min </li>
		</ul>
	   </li>
    </ul>
	</div>
	
	
	
    <!-- note Device Memory allocation can also be expensive -->
	</section>
	
	<section>
	<h4> Some planning will help a lot</h4>
	<ul>
	<li> Things to think about:
    <ul class = "inner">
       <li> How are you going to move data around?</li>
       <li> How are you going to determine if the answer is correct?</li>
       <li> How are you going to profile it?</li>
    </ul>

	<li> Reductions </li>
    <ul class = "inner">
       <li> What happens in the first step of a matrix-vector? </li>
       <li> Does it matter what type of memory you use to hold the results?</li>
	   <li> Can CUDA help us with this?</li>
    </ul>
	</ul>
	</section>
	
	<section>
	<h4> Reductions </h4>

    <ul class = "inner">
	<li>What does the below do on the GPU?</li>
	<li>How does each thread (recall they operate roughly in lockstep) know which value of temp it is reading</li>
	<li>Worse how does any thread know when to write, or what to write?</li>
	<li>This is called a reduction in most parallel languages, vector tempa[] is reduced into scalar temp with a + operator</li>
    </ul>
	
    <p>
    <pre class = "stretch"><code class = "cpp" data-trim>
    __shared__ int temp;
    __shared__ int tempa[THREADS_PER_BLOCK];
	v_idx = threadIdx.x;
	for (int i = 0; i < v_idx; i++){
		temp += tempa[i];
	}
	
    </code></pre></p>
	</section>
	
	<section>
	<h4> AtomicAdd </h4>

    <ul class = "inner">
	<li>Enter, your new best friend, AtomicAdd...</li>
	<li>Forces the threads to behave sensibly by adding their components in order</li>
	<li>Not the fastest method to do this, but it is a CUDA function, so likely to be as fast as possible</li>
	<li>Note that the implementation of this does NOT zero the value before starting!</li>
    </ul>

    <p>
    <pre class = "stretch"><code class = "cpp" data-trim>
	function(..., float *result){
	...
	t_idx = threadIdx.x;
	__shared__ int temp_array[THREADS_PER_BLOCK];
	atomicAdd(result, temp_array[t_idx]);
    </code></pre></p>
	
	</section>


	<section>
	<h4> Exercise time </h4>
	<ul class=outer">
	<li>You can find this exercise in the usual place, under single</li>
	<li>My full (ie with MV and V.V) implemented is in the single-solution directory</li>
	<li>Try the following pieces of exercise:</li>

		
	</ul>
	
	<div class="lblock">
    <ul class = "inner">
	<li> 1.1 Create a vector dot product kernel (vector_vector) </li>
	<li> 1.2 Create a matrix vector kernel (matrix_vector) </li>
	<li> 1.3 Create the vector mins/add (factor) vector kernels </li>
	<li> 2.1 Profile these kernels to see the effect of atomicAdd being called by all threads in a block adds</li>
	<li> 2.2 Can you cut this down by doing it on a single thread?
	
   </ul>
   </div>
   
	<div class="rblock">	
	<ul class = "inner">
	<li> 3.1 Add a finishing condition (sqrt(R) < 1e-5 ?)
	<li> 4.1 Increase the size of your vectors and matrices until you fill the GPU
	<li> 4.2 Compare the performance in terms of run-time for the kernels vs copying time for the data
	<li> 5.1 Optimise by removing un-necessary copy-backs and re-profile
	</ul>
	
    
	</div>

   
	</section>

	<section>
	<h4>Hints</h4>
	My demo code in the slides for atomicAdd could be reframed to
	
	<p>
    <pre class = "stretch"><code class = "cpp" data-trim>
	v_idx = threadIdx.x;
	if (v_idx == 0){
		float sum = 0;
		for (i = 0; i < THREADS_PER_BLOCK; i++){
			sum += temp_array[v_idx];
		}
		atomicAdd(result, sum);
	}
    </code></pre></p>
	</section>
	
	<section>	
	<h4>Hints</h4>
	I also do a horrible thing in that I use a point to scalar as the return method from a function
	
	<p>
    <pre class = "stretch"><code class = "cpp" data-trim>
	__global__ void function(float *A, float *B, float *scalar)
    </code></pre></p>
	
	If you combine this with an atomicAdd, you could end up in trouble. Why?
	
	<p>
    <pre class = "stretch"><code class = "cpp" data-trim>
	__global__ void function(float *A, float *B, float *scalar)
	...
	atomicAdd(scalar, values[index]);
	...
    </code></pre></p>
	</section>	
	
	<section>	
	<h4>Hints</h4>
	I do too many copies.

    <ul class = "inner">
	<li> Does the initialisation need to be done on the GPU? </li>
	<li> Do we need vector B on the GPU? </li>
	<li> Do we need to copy everything back to the host, at each step, for a single GPU? </li>
	<li> If we are worried about space, can we abuse existing space to hold intermediate products? </li>
	<li> Could we combine some steps? </li>
	
    </ul>

	
	
	</section>	
	

  

</div>
</div>

<!-- End of presentation -->

<script src="../lib/js/head.min.js"></script>
<script src="../js/reveal.js"></script>

<script>
// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  controls: false,
  slideNumber: true,
  center: false,
  math: { mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full'
         // See http://docs.mathjax.org/en/latest/config-files.html
        },
  dependencies: [
	{ src: '../plugin/markdown/marked.js' },
	{ src: '../plugin/markdown/markdown.js' },
	{ src: '../plugin/notes/notes.js', async: true },
        { src: '../plugin/math/math.js', async: true},
	{ src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
		]
});
</script>

</body>
</html>
